!sudo apt-get install -y pciutils
!curl -fsSL https://ollama.com/install.sh | sh # download ollama api
from IPython.display import clear_output

# Create a Python script to start the Ollama API server in a separate thread

import os
import threading
import subprocess
import requests
import json

def ollama():
    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'
    os.environ['OLLAMA_ORIGINS'] = '*'
    subprocess.Popen(["ollama", "serve"])

ollama_thread = threading.Thread(target=ollama)
ollama_thread.start()
     

from IPython.display import clear_output
!ollama pull llama3.1:8b
clear_output()

!pip install -U lightrag[ollama]

from lightrag.core.generator import Generator
from lightrag.core.component import Component
from lightrag.core.model_client import ModelClient
from lightrag.components.model_client import OllamaClient, GroqAPIClient

import time


qa_template = r"""
You are a helpful assistant.

User: {{input_str}}
You:"""

class SimpleQA(Component):
    def __init__(self, model_client: ModelClient, model_kwargs: dict):
        super().__init__()
        self.generator = Generator(
            model_client=model_client,
            model_kwargs=model_kwargs,
            template=qa_template,
        )

    def call(self, input: dict) -> str:
        return self.generator.call({"input_str": str(input)})

    async def acall(self, input: dict) -> str:
        return await self.generator.acall({"input_str": str(input)})

from lightrag.components.model_client import OllamaClient
from IPython.display import Markdown, display
model = {
    "model_client": OllamaClient(),
    "model_kwargs": {"model": "llama3.1:8b"}
}
# Assuming there's code before this snippet that defines 'model' and 'SimpleQA'
# and that the '}' is erroneous.

qa = SimpleQA(**model)
output=qa("""Consider the case **Ramana Dayaram Shetty vs The International Airport Authority of India** decided on May 4, 1979. Your task is to reconstruct the case proceedings in a detailed, step-by-step narrative format.
For each significant part of the case:")
1. Present the case proceedings in a clear and engaging way.
2. Ask an analytical or opinion-based question related to Athe preceding part. These questions should be concise and fall into one of the following categories:
   - Agree/Disagree type
   - Analytical opinion type (e.g., "Who do you think is right? or Do you think this was moral according to this or that law")
   - Predictive questions (e.g., "What do you think could happen next this or that ? ")
3. Keep it interactive like the user feels immersed in the case
4. Do not wait for user answers; proceed directly to the next part of the case after each question.
5. Conclude the output with a summary of the case, its key points, and the actual verdict delivered by the court.

Ensure a conversational tone that engages the user and guides them through the case while prompting thought and reflection on the legal, ethical, and practical dimensions of the case.""")
display(Markdown(f"**Answer:** {output.data}"))


!pip install flask flask-cors
!pip install flask flask-cors pyngrok

from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok

!pip install pyngrok
from pyngrok import ngrok

app = Flask(__name__)
CORS(app)  # Enable Cross-Origin Resource Sharing


import os
import threading
import subprocess
import requests
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok

# Flask app setup
app = Flask(__name__)
CORS(app)

# Ngrok setup
public_url = ngrok.connect(5000)
print(f"Public URL for testing endpoints: {public_url}")

# Dummy Ollama generator simulation for testing
@app.route('/generate', methods=['POST'])
def generate_response():
    # Get user input from Postman
    data = request.json
    user_input = data.get('input', '')  # Example: {"input": "Test case data"}

    # Simulate Ollama's output
    ollama_output = f"Ollama's response to: {user_input}"
    return jsonify({'output': ollama_output})  # Response JSON

# Endpoint to send data to Spring Boot
@app.route('/send_to_spring', methods=['POST'])
def send_to_spring():
    # Capture Ollama output
    data = request.json  # Expect JSON with the Ollama output
    ollama_output = data.get("output", "")  # Extract the "output" field

    # Spring Boot endpoint
    spring_url = "http://localhost:8080/api/ml/generate"  # Update with actual endpoint
    spring_payload = {
        "ollama_data": ollama_output  # Key must match the expected field in Spring Boot
    }

    # Send POST request to Spring Boot
    try:
        response = requests.post(spring_url, json=spring_payload)
        return jsonify({"message": "Data sent successfully", "spring_response": response.json()})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Start Flask server
thread = threading.Thread(target=app.run, kwargs={'host': '0.0.0.0', 'port': 5000})
thread.start()
